
Adversarial Training System
ğŸ”’ Making AI Models Resilient Against Adversarial Attacks

ğŸ“Œ Overview
The Adversarial Training System is an innovative solution designed to strengthen AI models against adversarial attacks like FGM, PGD, CarliniL2, and DeepFool. These attacks exploit AI model vulnerabilities, leading to incorrect predictions and security risks. Our system enhances model robustness by retraining it with adversarial examples, making it more secure and reliable.

ğŸš€ Features
âœ” Automatic Model Security Enhancement â€“ Upload a model, and the system will generate a more secure version.
âœ” Supports Multiple Attack Types â€“ Detects and protects against various adversarial attacks.
âœ” Customizable Security Levels â€“ Choose from different security levels (Fast, Low, Medium, High).
âœ” Intuitive UI â€“ A Streamlit-based interface for seamless interaction.
âœ” Performance Visualization â€“ Graphs showing accuracy and loss trends before and after training.

ğŸ› Tech Stack
Programming Language: Python
Framework: Streamlit
Deep Learning: PyTorch
Adversarial AI: Adversarial Robustness Toolbox (ART)
Data Handling: NumPy, Pandas
Visualization: Matplotlib



ğŸ“‚ Project Structure

ğŸ“‚ Adversarial-Training-System  
 â”œâ”€â”€ ğŸ“‚ uploads              # Folder to store uploaded models & datasets  
 â”œâ”€â”€ ğŸ“„ main.py              # Main script handling model training & UI  
 â”œâ”€â”€ ğŸ“„ requirements.txt     # List of dependencies  
 â”œâ”€â”€ ğŸ“„ README.md            # Project documentation  
 â”œâ”€â”€ ğŸ“„ secure_model.pth     # Example secured model output  
 
ğŸ”§ Installation & Setup
1ï¸âƒ£ Clone the Repository
git clone https://github.com/your-repo/adversarial-training-system.git
cd adversarial-training-system
2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt
3ï¸âƒ£ Run the Application
streamlit run main.py

ğŸ¯ How It Works
1ï¸âƒ£ Upload your pre-trained AI model (.pth/.pt) and dataset (CSV format).
2ï¸âƒ£ Select the security level â€“ Choose between Fast, Low, Medium, or High.
3ï¸âƒ£ The system generates adversarial examples using attacks like PGD, CarliniL2, etc.
4ï¸âƒ£ Retrains the model with adversarial and clean data to improve robustness.
5ï¸âƒ£ Download the secured model, now immune to adversarial attacks.
6ï¸âƒ£ View accuracy and loss trends via interactive plots.


ğŸ“Œ Why This Matters?
With AI being integrated into healthcare, finance, and autonomous systems, security is more crucial than ever. 
Adversarial attacks can manipulate AI decisions, leading to critical failures. This system ensures that AI models remain trustworthy and resilient against such threats.

ğŸ¤ Contributors
ğŸ‘¤ Krish
ğŸ‘¤ Aaryan Joshi
ğŸ‘¤ Devansh Kapadia

ğŸŒŸ Future Enhancements
âœ… Extend support to different neural network architectures.
âœ… Implement real-time attack detection for deployed AI models.
âœ… Optimize training efficiency for faster adversarial training.

ğŸ“œ License
This project is open-source under the MIT License.

â­ Show Some Love!
If you find this project helpful, consider giving it a â­ on GitHub! ğŸš€
